\input{header.tex}
% -----------------------------------------------------------------------------
\title{Exercise sheet 9}
\subtitle{Numerical Optimization: CG, Newton's method}
% =============================================================================
\begin{document}
\maketitle

\paragraph{Exercise 9.1}  % Nocedal 5.2
Suppose that $p_0,\dots,p_l$ are conjugate. Let us express one of them, say
$p_i$, as a linear combination of the others,
\begin{equation}\label{eq:lin}
p_i = \sigma_0 p_0 + \dots + \sigma_k p_l
\end{equation}
for some coefficients $\sigma_k, k\in\{0, 1, \dots, l\}$. Note that the sum does not include $p_i$. Then from conjugacy, we have
\[
0
= p_0^\tp A p_i
= \sigma_0 p_0^\tp A p_0 + \dots + \sigma_l p_0^\tp A p_l
= \sigma_0 p_0^\tp A p_0.
\]
This implies that $\sigma_0 = 0$ since the vectors $p_0, \dots, p_l$ are
assumed to be conjugate and $A$ is positive definite. The same argument is
used to show that all the scalar coefficients $\sigma_k, k\in\{0,\dots,l\}$ in
(\ref{eq:lin}) are zero. Equation (\ref{eq:lin}) indicates that $p_i = 0$,
which contradicts the fact that $p_i$ is a nonzero vector. The contradiction
then shows that vectors $p_0,\dots, p_l$ are linearly independent.



\paragraph{Exercise 9.2}  % Nocedal 5.4
To see that $h(\sigma) = f (x_0 + \sigma_0 p_0 +\dots+ \sigma_{k−1} p_{k−1})$ is quadratic, note that
\[
\sigma_0 p_0 + \dots + \sigma_{k−1} p_{k−1} = P\sigma
\]
where $P$ is the $n \times k$-matrix whose columns are the $n\times 1$-vectors $p_i$ , i.e.,
\[
P =
\begin{pmatrix}
\vline & & \vline\\
p_0 & \dots & p_{k-1}\\
\vline & & \vline
\end{pmatrix}
\]
and $\sigma$ is the $k \times 1$-matrix
\[
\sigma = (\sigma_0,\dots,\sigma_{k-1})^\tp.
\]
Therefore
\[
\begin{split}
h(\sigma)
&= \frac{1}{2}(x_0 + P\sigma)^\tp A (x_0+P\sigma) + b^\tp (x_0 + P\sigma)\\
&= \frac{1}{2} x_0^\tp A x_0 + x_0^\tp A P \sigma + \frac{1}{2}\sigma^\tp P^\tp AP\sigma + b^\tp x_0 + b^\tp P\sigma\\
&= \frac{1}{2} x_0^\tp A x_0 + b^\tp x_0 + [P^\tp A^\tp x_0 + P^\tp b]^\tp\sigma + \frac{1}{2}\sigma^\tp (P^\tp A P)\sigma\\
&= C + \hat{b}^\tp \sigma + \sigma^\tp \hat{A}\sigma,
\end{split}
\]
where
\[
C \dfn\frac{1}{2} x_0^\tp A x_0 + b^\tp x_0,
\quad
\hat{b}\dfn P^\tp A^\tp x_0 + P^\tp b,
\quad
\hat{A} \dfn P^\tp A P.
\]
If the set of vectors $\{p_0,\dots, p_{k-1}\}$ is linearly independent, then
$P$ has full rank and $\hat{A}$ is positive definite. Hence $h(\cdot)$ is
strictly convex quadratic.


\paragraph{Exercise 9.3}
Newton's method is applied to the function $f(x)=x^m$ with \emph{even}
$m\in\N$. What can you say about the rate of convergence?

\paragraph{Exercise 9.4}
Let $x \in \R^n$ and $f$ be twice continuously differentiable. Show that $p =
-H\nabla f(x)$ is a descent direction of $f$ at $x$ for any positive definite
matrix $H$ if $\nabla f(x) \neq 0$.

\paragraph{Exercise 9.5}
% http://www.uni-graz.at/imawww/hintermueller/opt_I_10.pdf
Consider the minimization problem
\[
  f(x)\dfn \frac{1}{2}\sum_{i=1}^M\|r_i\|^2 = \frac{1}{2}R(x)^\tp R(x) \to \min_{x\in\R^n}
\]
with $R(x)=(r_1,\dots,r_M)^\tp(x)$, where the functions $r_i:\R^n\to\R$, $i\in\{1,\dots,M\}$, are twice continuously differentiable. For a given $x_a$, define the following model for $f$ around $x_a$:
\[
  m_a(x) = f(x_a)
         + R(x_a)^\tp R'(x_a)(x-x_a)
         + \frac{1}{2} (x-x_a)^\tp R'(x_a)^\tp R'(x_a)(x-x_a).
\]
Based on $m_a$, define -- under appropriate conditions --, the analogue of
Newton's method. How is $m_a$ different from the ordinary second-order
approximation of $f$ in the defintion of Newton's method? Consider the cases
$M=n$, $M>n$, $M<n$. Which speed of convergence can be expected if $\nabla
f(x^*)\neq 0$?

\paragraph{Programming 9}
Based on \texttt{steepestDescent.m}, implement Newton's method.

Compare
\begin{itemize}
  \item the method of steepest descent,
  \item the conjugate-gradient method, and
  \item Newton's method
\end{itemize}
for some test problems (e.g., the Rosenbrock function). Plot the residual
norms $\|\nabla f(x_k)\|$ as well as the iterates $x_k$. What do you observe?

\end{document}
% =============================================================================
