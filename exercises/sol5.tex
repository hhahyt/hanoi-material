\input{header.tex}
% -----------------------------------------------------------------------------
\title{Solutions for exercise sheet 5}
\subtitle{Numerical Optimization: Basics}
% =============================================================================
\begin{document}
\maketitle

\paragraph{Exercise 5.1} % Nocedal 2.1
\begin{itemize}
\item \[
\nabla f(x) =
\begin{pmatrix}
-400 x(y-x^2)-2(1-x)\\
200(y-x^2)
\end{pmatrix}
\]
\item \[
\nabla^2 f(x) =
\begin{pmatrix}
-400x (y-3x^2) + 2 & -400x\\
-400x              & 200
\end{pmatrix}
\]
\item $\nabla f(x^*) = 0$, $x*=(1,1)^\tp$ is the only solution.
\item
\[
\nabla^2 f(x*) =
\begin{pmatrix}
802 & -400\\
-400 & 200
\end{pmatrix}
\]
is positive definite since $802>0$ and $\det(\nabla^2 f(x^*)) = 802\cdot200 - 400\cdot 400 > 0$ (Sylvester's criterion).
\end{itemize}


\paragraph{Exercise 5.2} % Nocedal 2.2
\begin{itemize}
\item
\[
\nabla f(x) =
\begin{pmatrix}
8 + 2x\\
12 - 4y
\end{pmatrix}.
\]
The unique solution of $\nabla f(x^*) = 0$ is $x^*=(-4,3)^\tp$.
\item
\[
\nabla^2 f(x) =
\begin{pmatrix}
2 & 0\\
0 & - 4
\end{pmatrix}.
\]
The eigenvalues of $\nabla^2 f(x^*)$ are $\{2, -4\}$, so it is not positive-definite. Hence, $x^*$ is not a minimizer of $f$. With the same argument for $-f$ we see that $x^*$ neither is a minimizer of $-f$, i.e., a maximizer of $f$. Hence, $x^*$ is a saddle point.

% TODO plot
\end{itemize}

\paragraph{Exercise 5.3} % Nocedal 2.3
\begin{itemize}
\item
\[
\nabla f_1(x) = \sum_{i=1}^n a_i x_i,
\]
\[
\nabla f_1(x) =
\begin{pmatrix}
\partial f_1/\partial x_1\\
\vdots\\
\partial f_1/\partial x_n
\end{pmatrix}
=
\begin{pmatrix}
a_1\\
\vdots\\
a_n
\end{pmatrix}
=
a
\]
\[
(\nabla^2 f_1(x))_{i,j}
= \frac{\partial^2 f_1}{\partial x_i \partial x_j}(x)
= \frac{\partial a_i}{\partial x_j}
= 0.
\]

\item
\[
f_2(x) = \sum_{i=1}^n \sum_{j=1}^n x_i A_{i,j} x_j
\]
\[
(\nabla f_2(x))_s
= \partial f_2/\partial x_s
= \sum_{j=1}^n A_{s,j} x_j + \sum_{i=1}^n x_s A_{i,s}
= 2\sum_{j=1}^n A_{s,j} x_j,
\]
so $\nabla f_2(x) = 2Ax$.
\[
(\nabla^2 f_2(x))_{s, t}
= \frac{\partial^2 f_2}{\partial x_s \partial x_t}(x)
= A_{s,t} + A_{t,s}
= 2 A_{s,t}
\]
so $\nabla^2 f_2(x) = 2A$.
\end{itemize}

\paragraph{Exercise 5.4} % Nocedal 2.7
Let $S$ be the set of global minimizers of $f$. If $S$ only has one element,
then it is obviously a convex set. Otherwise for all $x, y \in S$ and
$\alpha\in [0, 1]$,
\[
f (\alpha x + (1 - \alpha)y) \le \alpha f(x) + (1 - \alpha)f (y)
\]
since $f$ is convex. $f(x) = f(y)$ since $x, y$ are both global minimizers.
Therefore,
\[
f (\alpha x + (1 - \alpha)y) \le \alpha f (x) + (1 - \alpha)f (x) = f (x).
\]
But since $f (x)$ is a global minimizing value, $f (x) \le f (\alpha x + (1 -
\alpha)y)$.  Therefore, $f (\alpha x + (1 - \alpha y) = f (x)$ and hence
$\alpha x + (1 - \alpha )y \in S$. Thus $S$ is a convex set.


\paragraph{Exercise 5.5} % Nocedal 2.8
Consider the function
\[
  f(x_1, x_2) = (x_1+x_2^2)^2.
\]
At the point $x=(1,0)^\tp$ we consider the search direction $p=(-1,1)^\tp$.
Show that $p$ is a descent direction and find all minimizers of the problem.

$−\nabla f$ indicates steepest descent. $p_k\cdot · (−\nabla f) = \|p_k\| \|\nabla f\| \cos\theta$. $p_k$ is a descent direction if $−\pi/2 < \theta < \pi /2$, i.e., $\cos \theta > 0$. This means that
\[
0 \stackrel{!}{<} \cos\theta = \frac{p_k\cdot (-\nabla f)}{\|p_k\|\|\nabla f\|},
\]
so we have to check if $p_k\cdot\nabla f < 0$:
\[
\nabla f(x) =
\begin{pmatrix}
2(x+y^2)\\
4y(x+y^2)
\end{pmatrix},
\quad
\nabla f((1,0)^\tp) =
\begin{pmatrix}
2\\
0
\end{pmatrix}.
\]
\[
p_k \cdot \nabla f
=
(-1, 1)^\tp \cdot (2,0)^\tp = -2 < 0.
\]
so indeed, $p$ is a descent direction at $(1,0)^\tp$.

The ideal step size $\alpha^*$ is the value that minimizes $f(x_k+\alpha
p_k)$. In this case,
\[
g(\alpha) \dfn f(x_k + \alpha p_k) = f((1-\alpha,\alpha)^\tp) = ((1-\alpha)+\alpha^2)^2.
\]
We have
\[
\frac{\dd g}{\dd\alpha} = 2(1-\alpha+\alpha^2)(-1+2\alpha),
\]
which is $0$ for $\alpha\in\{1/2, 1/2\pm\ii\sqrt{1/2}\}$. We are only
interested in real-valued solutions, so let $\alpha^*=1/2$. Indeed, we have
\[
\left.\frac{\dd^2 g}{\dd\alpha^2}\right|_{\alpha=\alpha^*}
= \left.6(2\alpha^2-2\alpha+1)\right|_{\alpha=\alpha^*}
= 3 > 0,
\]
so $\alpha^*$ is indeed a minimizer.

\paragraph{Exercise 5.6} % Nocedal 2.9

We have
\[
x_j = \sum_{i=1}^n S_{j,i}z_i  + s_j.
\]
From this follows that
\[
\frac{\partial \tilde{f}}{\partial z_i}
= \sum_{j=1}^n \frac{\partial f}{\partial x_j} \frac{\partial x_j}{\partial z_i}
= \sum_{j=1}^n \frac{\partial f}{\partial x_j} S_{j,i}
= (S^\tp\nabla f(x))_i.
\]
The same works for the second derivative:
\[
\frac{\partial^2 \tilde{f}}{\partial z_i\partial z_k}
= \frac{\partial}{\partial z_k}\sum_{j=1}^n \frac{\partial f}{\partial x_j} S_{j,i}
= \sum_{j=1}^n\sum_{l=1}^n \frac{\partial^2 f}{\partial x_j \partial x_l} S_{l,k} S_{j,i}
= (S^\tp \nabla^2 f(x)S)_{k,i}.
\]

\end{document}
% =============================================================================
