\input{header.tex}
% -----------------------------------------------------------------------------
\title{Exercise sheet 9}
\subtitle{Numerical Optimization: CG, Newton's method}
% =============================================================================
\begin{document}
\maketitle

\paragraph{Exercise 9.1}  % Nocedal 5.2
Show that if the nonzero vectors $p_0, p_1,\dots, p_k$ satisfy (??), where $A$
is symmetric and positive-definite, then these vectors are linearly
independent. (The results implies that $A$ has at most $n$ conjugate
directions.)

\paragraph{Exercise 9.2}  % Nocedal 5.4
Show that if $f(\cdot)$ is a strictly convex quadratic function, the function
$h(\sigma)=f(x_0+\sigma_0 p_0 + \dots \sigma_{k-1}p_{k-1})$ also is a strictly
convex quadratic function in $\sigma=(\sigma_0,\dots,\sigma_{k-1})^\tp$.

\paragraph{Exercise 9.3}
Newton's method is applied to the function $f(x)=x^m$ with \emph{even}
$m\in\N$. What can you say about the rate of convergence?

\paragraph{Exercise 9.4}
Let $x \in \R^n$ and $f$ be twice continuously differentiable. Show that $p =
-H\nabla f(x)$ is a descent direction of $f$ at $x$ for any positive definite
matrix $H$ if $\nabla f(x) \neq 0$.


% Hinterm√ºller, (P3)
% <http://www.uni-graz.at/imawww/hintermueller/opt_I_10.pdf>
\paragraph{Programming 9}
Based on \texttt{steepestDescent.m}, implement Newton's method.

Compare
\begin{itemize}
  \item the method of steepest descent,
  \item the conjugate-gradient method, and
  \item Newton's method
\end{itemize}
for some test problems (e.g., the Rosenbrock function). Plot the residual
norms $\|\nabla f(x_k)\|$ as well as the iterates $x_k$. What do you observe?

\end{document}
% =============================================================================
