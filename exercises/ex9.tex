\input{header.tex}
% -----------------------------------------------------------------------------
\title{Exercise sheet 9}
\subtitle{Numerical Optimization: CG, Newton's method}
% =============================================================================
\begin{document}
\maketitle

\paragraph{Exercise 9.1}  % Nocedal 5.2
Show that if the nonzero vectors $p_0, p_1,\dots, p_k$ satisfy
\[
  p_i^\tp A p_j = 0 \quad\forall i\neq j,
\]
where $A$ is symmetric and positive-definite, then these vectors are linearly
independent. (The result implies that $A$ has at most $n$ conjugate
directions.)

\paragraph{Exercise 9.2}  % Nocedal 5.4
Show that if $f(\cdot)$ is a strictly convex quadratic function, the function
$h(\sigma)=f(x_0+\sigma_0 p_0 + \dots +\sigma_{k-1}p_{k-1})$ also is a strictly
convex quadratic function in $\sigma=(\sigma_0,\dots,\sigma_{k-1})^\tp$.

\paragraph{Exercise 9.3}
Newton's method is applied to the function $f(x)=x^m$ with \emph{even}
$m\in\N$. What can you say about the rate of convergence?

\paragraph{Exercise 9.4}
Let $x \in \R^n$ and $f$ be twice continuously differentiable. Show that $p =
-H^{-1}\nabla f(x)$ is a descent direction of $f$ at $x$ for any
positive-definite matrix $H$ if $\nabla f(x) \neq 0$.

\paragraph{Exercise 9.5}
% http://www.uni-graz.at/imawww/hintermueller/opt_I_10.pdf
Consider the minimization problem
\[
  f(x)\dfn \frac{1}{2}\sum_{i=1}^M\|r_i\|^2 = \frac{1}{2}R(x)^\tp R(x) \to \min_{x\in\R^n}
\]
with $R(x)=(r_1,\dots,r_M)^\tp(x)$, where the functions $r_i:\R^n\to\R$, $i\in\{1,\dots,M\}$, are twice continuously differentiable. For a given $x_a$, define the following model for $f$ around $x_a$:
\[
  m_a(x) = f(x_a)
         + R(x_a)^\tp R'(x_a)(x-x_a)
         + \frac{1}{2} (x-x_a)^\tp R'(x_a)^\tp R'(x_a)(x-x_a).
\]
Based on $m_a$, define -- under appropriate conditions --, the analogue of
Newton's method. How is $m_a$ different from the ordinary second-order
approximation of $f$ in the defintion of Newton's method? Consider the cases
$M=n$, $M>n$, $M<n$. Which speed of convergence can be expected if $f(x^*)\neq 0$?

\paragraph{Programming 9}
Based on \texttt{steepestDescent.m}, implement Newton's method.

Compare
\begin{itemize}
  \item the method of steepest descent,
  \item the conjugate-gradient method, and
  \item Newton's method
\end{itemize}
for some test problems (e.g., the Rosenbrock function). Plot the residual
norms $\|\nabla f(x_k)\|$ as well as the iterates $x_k$. What do you observe?

\end{document}
% =============================================================================
