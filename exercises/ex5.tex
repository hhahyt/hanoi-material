\input{header.tex}
% -----------------------------------------------------------------------------
\title{Exercise sheet 5}
\subtitle{Numerical Optimization: Basics}
% -----------------------------------------------------------------------------
\usepackage{amsmath}
% -----------------------------------------------------------------------------
\newcommand\tp{\ensuremath{\text{\upshape T}}}
% =============================================================================
\begin{document}
\maketitle

\paragraph{Exercise 1.1} % Nocedal 2.1
Compute  the gradient $\nabla f(x)$ and Hessian $\nabla^2f(x)$ of the
\emph{Rosenbrock function}
\begin{equation}\label{eq:rosenbrock}
  f(x) = 100(x_2-x_1^2)^2 + (1-x_1)^2.
\end{equation}
Show that $x^*=(1,1)^\tp$ is the only local minimizer of this function, and
that the Hessian matrix at that point is positive-definite.

\paragraph{Exercise 1.2} % Nocedal 2.2
Show that the function
\[
  f(x) = 8x_1 + 12x_2 + x_1^2 - 2x_2^2
\]
has only one stationary point, and that it is neither a maximum nor a minimum,
but a saddle point. Sketch the contour lines of $f$.

\paragraph{Exercise 1.3} % Nocedal 2.7
Suppose that $f$ is a convex function. Show that the set of global minimizer
of $f$ is a convex set.

\paragraph{Exercise 1.4} % Nocedal 2.8
Consider the function
\[
  f(x_1, x_2) = (x_1+x_2^2)^2.
\]
At the point $x=(1,0)^\tp$ we consider the search direction $p=(-1,1)^\tp$.
Show that $p$ is a descent direction and find all minimizers of the problem.

\paragraph{Programming 1}  % Nocedal 3.1
Program
\begin{itemize}
  \item the steepest descent method and
  \item Newton's method
\end{itemize}
with the backtracking line search.

Use them to minimize
\begin{itemize}
  \item the Rosenbrock function (\ref{eq:rosenbrock}),
  \item the \emph{sphere function}
    \[
      f(x) = \sum_{i=1}^n x_i^2,
    \]
    and
  \item one function of your choice from
    \url{https://en.wikipedia.org/wiki/Test_functions_for_optimization}.
\end{itemize}
Set the initial step length $\alpha_0=1$ and print the step length used at
each method at each iteration. First try the initial point $x_0=(1.2,
1.2)^\tp$, and then the more difficult point $x_0=(-1.2, 1)^\tp$.

% TODO add Hintermueller exercises?

\end{document}
% =============================================================================
